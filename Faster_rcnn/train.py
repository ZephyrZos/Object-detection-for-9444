
import os
import sys
import time
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.tensorboard import SummaryWriter
from torch.optim.lr_scheduler import StepLR, CosineAnnealingLR, ReduceLROnPlateau
import numpy as np
from tqdm import tqdm
import random
from pathlib import Path
import glob

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from convert_dataset import convert_visdrone_to_coco
from dataset import create_data_loaders
from model import create_model, save_model, load_pretrained_model, ModelEMA, print_model_summary
from utils import AverageMeter, EarlyStopping, get_device


def set_seed(seed):
    """è®¾ç½®éšæœºç§å­"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


def find_latest_checkpoint(checkpoint_dir="checkpoints"):
    """æŸ¥æ‰¾æœ€æ–°çš„æ£€æŸ¥ç‚¹æ–‡ä»¶"""
    if not os.path.exists(checkpoint_dir):
        return None

    # æŸ¥æ‰¾æ‰€æœ‰æ£€æŸ¥ç‚¹æ–‡ä»¶
    checkpoint_patterns = [
        f"{checkpoint_dir}/faster_rcnn_*/best_model.pth",
        f"{checkpoint_dir}/faster_rcnn_*/final_model.pth",
        f"{checkpoint_dir}/faster_rcnn_*/checkpoint_epoch_*.pth"
    ]

    all_checkpoints = []
    for pattern in checkpoint_patterns:
        all_checkpoints.extend(glob.glob(pattern))

    if all_checkpoints:
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
        latest_checkpoint = max(all_checkpoints, key=os.path.getmtime)
        return latest_checkpoint

    return None


def load_checkpoint(checkpoint_path, model, optimizer, scheduler, device):
    """åŠ è½½æ£€æŸ¥ç‚¹"""
    print(f"ğŸ”„ åŠ è½½æ£€æŸ¥ç‚¹: {checkpoint_path}")

    checkpoint = torch.load(checkpoint_path, map_location=device)

    # åŠ è½½æ¨¡å‹æƒé‡
    if "model_state_dict" in checkpoint:
        model.load_state_dict(checkpoint["model_state_dict"])
    else:
        model.load_state_dict(checkpoint)

    # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
    if "optimizer_state_dict" in checkpoint:
        optimizer.load_state_dict(checkpoint["optimizer_state_dict"])

    # åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€
    if "scheduler_state_dict" in checkpoint and scheduler is not None:
        scheduler.load_state_dict(checkpoint["scheduler_state_dict"])

    # è·å–è®­ç»ƒçŠ¶æ€
    start_epoch = checkpoint.get("epoch", 0) + 1  # ä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
    best_loss = checkpoint.get("loss", float("inf"))

    print(f"âœ… æ£€æŸ¥ç‚¹åŠ è½½æˆåŠŸ")
    print(f"   å¼€å§‹epoch: {start_epoch}")
    print(f"   æœ€ä½³æŸå¤±: {best_loss:.4f}")

    return start_epoch, best_loss


def create_optimizer(model, config):
    """åˆ›å»ºä¼˜åŒ–å™¨"""
    optimizer_name = config["training"].get("optimizer", "sgd").lower()
    lr = config["training"]["learning_rate"]
    weight_decay = config["training"]["weight_decay"]
    momentum = config["training"]["momentum"]

    if optimizer_name == "sgd":
        optimizer = optim.SGD(
            model.parameters(), lr=lr, momentum=momentum, weight_decay=weight_decay
        )
    elif optimizer_name == "adam":
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)
    elif optimizer_name == "adamw":
        optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„ä¼˜åŒ–å™¨: {optimizer_name}")

    return optimizer


def create_scheduler(optimizer, config, num_steps):
    """åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨"""
    scheduler_name = config["training"].get("lr_scheduler", "step").lower()

    if scheduler_name == "step":
        scheduler = StepLR(
            optimizer,
            step_size=config["training"]["lr_step_size"],
            gamma=config["training"]["lr_gamma"],
        )
    elif scheduler_name == "cosine":
        scheduler = CosineAnnealingLR(optimizer, T_max=config["training"]["epochs"])
    elif scheduler_name == "plateau":
        scheduler = ReduceLROnPlateau(
            optimizer,
            mode="min",
            factor=config["training"]["lr_gamma"],
            patience=config["training"].get("lr_patience", 5),
            # verboseå‚æ•°åœ¨æ–°ç‰ˆæœ¬PyTorchä¸­å·²ç§»é™¤
        )
    else:
        raise ValueError(f"ä¸æ”¯æŒçš„å­¦ä¹ ç‡è°ƒåº¦å™¨: {scheduler_name}")

    return scheduler


def train_one_epoch(model, train_loader, optimizer, device, epoch, config):
    """è®­ç»ƒä¸€ä¸ªepoch"""
    model.train()

    # æŸå¤±è®°å½•å™¨
    loss_meter = AverageMeter()
    rpn_loss_meter = AverageMeter()
    roi_loss_meter = AverageMeter()

    # è¿›åº¦æ¡
    pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

    try:
        for batch_idx, (images, targets) in enumerate(pbar):
            try:
                # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
                images = [image.to(device) for image in images]
                targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

                # æ£€æŸ¥æ•°æ®æœ‰æ•ˆæ€§
                if not images or not targets:
                    print(f"è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # å‰å‘ä¼ æ’­
                loss_dict = model(images, targets)

                # æ£€æŸ¥æŸå¤±å­—å…¸
                if not loss_dict:
                    print(f"è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} æŸå¤±å­—å…¸ä¸ºç©ºï¼Œè·³è¿‡")
                    continue

                # è®¡ç®—æ€»æŸå¤±
                losses = sum(loss for loss in loss_dict.values())

                # æ£€æŸ¥æŸå¤±æ˜¯å¦æœ‰æ•ˆ
                if torch.isnan(losses) or torch.isinf(losses):
                    print(f"è­¦å‘Š: æ‰¹æ¬¡ {batch_idx} å‡ºç°æ— æ•ˆæŸå¤±å€¼: {losses.item()}")
                    continue

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                losses.backward()

                # æ¢¯åº¦è£å‰ª
                if config["training"].get("gradient_clip_val", 0) > 0:
                    torch.nn.utils.clip_grad_norm_(
                        model.parameters(), config["training"]["gradient_clip_val"]
                    )

                optimizer.step()

                # æ›´æ–°æŸå¤±è®°å½•
                loss_meter.update(losses.item())

                # å®‰å…¨åœ°è·å–RPNå’ŒROIæŸå¤±
                rpn_loss = 0.0
                roi_loss = 0.0

                if "loss_objectness" in loss_dict and "loss_rpn_box_reg" in loss_dict:
                    rpn_loss = loss_dict["loss_objectness"].item() + loss_dict["loss_rpn_box_reg"].item()

                if "loss_classifier" in loss_dict and "loss_box_reg" in loss_dict:
                    roi_loss = loss_dict["loss_classifier"].item() + loss_dict["loss_box_reg"].item()

                rpn_loss_meter.update(rpn_loss)
                roi_loss_meter.update(roi_loss)

                # æ›´æ–°è¿›åº¦æ¡
                pbar.set_postfix({
                    'Loss': f'{loss_meter.avg:.4f}',
                    'RPN': f'{rpn_loss_meter.avg:.4f}',
                    'ROI': f'{roi_loss_meter.avg:.4f}'
                })

            except Exception as e:
                print(f"å¤„ç†æ‰¹æ¬¡ {batch_idx} æ—¶å‡ºé”™: {e}")
                continue

        return {
            'loss': loss_meter.avg,
            'rpn_loss': rpn_loss_meter.avg,
            'roi_loss': roi_loss_meter.avg
        }

    except Exception as e:
        print(f"è®­ç»ƒç¬¬{epoch+1}è½®æ—¶å‡ºé”™: {e}")
        print(f"é”™è¯¯ç±»å‹: {type(e).__name__}")
        import traceback
        traceback.print_exc()

        # è¿”å›é»˜è®¤å€¼é¿å…ç¨‹åºå´©æºƒ
        return {
            'loss': float('inf'),
            'rpn_loss': float('inf'),
            'roi_loss': float('inf')
        }


def validate(model, val_loader, device, config):
    """éªŒè¯æ¨¡å‹"""
    model.eval()

    total_loss = 0
    num_batches = 0

    # ç”¨äºè®¡ç®—mAPçš„åˆ—è¡¨
    all_predictions = []
    all_targets = []

    with torch.no_grad():
        for images, targets in tqdm(val_loader, desc="Validation"):
            # ç§»åŠ¨æ•°æ®åˆ°è®¾å¤‡
            images = [image.to(device) for image in images]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]

            # å‰å‘ä¼ æ’­ - åœ¨éªŒè¯æ—¶éœ€è¦è®¡ç®—æŸå¤±
            # ä¸´æ—¶åˆ‡æ¢åˆ°è®­ç»ƒæ¨¡å¼æ¥è®¡ç®—æŸå¤±
            model.train()
            loss_dict = model(images, targets)
            losses = sum(loss for loss in loss_dict.values())

            # åˆ‡æ¢å›è¯„ä¼°æ¨¡å¼
            model.eval()

            total_loss += losses.item()
            num_batches += 1

            # è·å–é¢„æµ‹ç»“æœç”¨äºè®¡ç®—mAP
            predictions = model(images)

            # æ”¶é›†é¢„æµ‹å’Œç›®æ ‡ç”¨äºè®¡ç®—æŒ‡æ ‡
            for pred, target in zip(predictions, targets):
                all_predictions.append(pred)
                all_targets.append(target)

    avg_loss = total_loss / num_batches

    # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
    metrics = calculate_validation_metrics(all_predictions, all_targets, config)

    return avg_loss, metrics


def calculate_iou(box1, box2):
    """è®¡ç®—ä¸¤ä¸ªè¾¹ç•Œæ¡†çš„IoU"""
    # boxæ ¼å¼: [x1, y1, x2, y2]
    x1_max = max(box1[0], box2[0])
    y1_max = max(box1[1], box2[1])
    x2_min = min(box1[2], box2[2])
    y2_min = min(box1[3], box2[3])

    # è®¡ç®—äº¤é›†é¢ç§¯
    intersection_width = max(0, x2_min - x1_max)
    intersection_height = max(0, y2_min - y1_max)
    intersection_area = intersection_width * intersection_height

    # è®¡ç®—å¹¶é›†é¢ç§¯
    box1_area = (box1[2] - box1[0]) * (box1[3] - box1[1])
    box2_area = (box2[2] - box2[0]) * (box2[3] - box2[1])
    union_area = box1_area + box2_area - intersection_area

    # é¿å…é™¤é›¶
    if union_area == 0:
        return 0.0

    return intersection_area / union_area


def calculate_strict_metrics(predictions, targets, iou_threshold=0.5):
    """
    ä¸¥æ ¼è®¡ç®—TPã€FPã€FNå’Œç›¸åº”çš„precisionã€recall

    Args:
        predictions: é¢„æµ‹ç»“æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'boxes', 'labels', 'scores'
        targets: çœŸå®æ ‡ç­¾åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ åŒ…å« 'boxes', 'labels'
        iou_threshold: IoUé˜ˆå€¼ï¼Œé»˜è®¤0.5

    Returns:
        dict: åŒ…å«TPã€FPã€FNã€precisionã€recallçš„å­—å…¸
    """
    total_tp = 0
    total_fp = 0
    total_fn = 0

    for pred, target in zip(predictions, targets):
        # è½¬æ¢ä¸ºnumpyæ•°ç»„ä¾¿äºå¤„ç†
        if len(pred['boxes']) == 0:
            pred_boxes = np.array([]).reshape(0, 4)
            pred_labels = np.array([])
            pred_scores = np.array([])
        else:
            pred_boxes = pred['boxes'].cpu().numpy() if hasattr(pred['boxes'], 'cpu') else pred['boxes']
            pred_labels = pred['labels'].cpu().numpy() if hasattr(pred['labels'], 'cpu') else pred['labels']
            pred_scores = pred['scores'].cpu().numpy() if hasattr(pred['scores'], 'cpu') else pred['scores']

        if len(target['boxes']) == 0:
            target_boxes = np.array([]).reshape(0, 4)
            target_labels = np.array([])
        else:
            target_boxes = target['boxes'].cpu().numpy() if hasattr(target['boxes'], 'cpu') else target['boxes']
            target_labels = target['labels'].cpu().numpy() if hasattr(target['labels'], 'cpu') else target['labels']

        # è®°å½•å“ªäº›GTå·²ç»è¢«åŒ¹é…
        gt_matched = np.zeros(len(target_boxes), dtype=bool)

        # å¯¹é¢„æµ‹æŒ‰ç½®ä¿¡åº¦æ’åº
        if len(pred_scores) > 0:
            sorted_indices = np.argsort(pred_scores)[::-1]  # é™åºæ’åˆ—
            pred_boxes = pred_boxes[sorted_indices]
            pred_labels = pred_labels[sorted_indices]
            pred_scores = pred_scores[sorted_indices]

        # éå†æ¯ä¸ªé¢„æµ‹æ¡†
        for pred_idx in range(len(pred_boxes)):
            pred_box = pred_boxes[pred_idx]
            pred_label = pred_labels[pred_idx]

            best_iou = 0.0
            best_gt_idx = -1

            # æ‰¾åˆ°ä¸å½“å‰é¢„æµ‹æ¡†IoUæœ€å¤§ä¸”ç±»åˆ«åŒ¹é…çš„GTæ¡†
            for gt_idx in range(len(target_boxes)):
                if gt_matched[gt_idx]:  # è¯¥GTå·²è¢«åŒ¹é…
                    continue

                target_box = target_boxes[gt_idx]
                target_label = target_labels[gt_idx]

                # ç±»åˆ«å¿…é¡»åŒ¹é…
                if pred_label != target_label:
                    continue

                # è®¡ç®—IoU
                iou = calculate_iou(pred_box, target_box)

                if iou > best_iou:
                    best_iou = iou
                    best_gt_idx = gt_idx

            # åˆ¤æ–­æ˜¯å¦ä¸ºTP
            if best_iou >= iou_threshold and best_gt_idx != -1:
                total_tp += 1
                gt_matched[best_gt_idx] = True  # æ ‡è®°è¯¥GTå·²è¢«åŒ¹é…
            else:
                total_fp += 1

        # è®¡ç®—FNï¼šæœªè¢«åŒ¹é…çš„GTæ¡†æ•°é‡
        total_fn += np.sum(~gt_matched)

    # è®¡ç®—precisionå’Œrecall
    precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0
    recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0

    return {
        'TP': total_tp,
        'FP': total_fp,
        'FN': total_fn,
        'precision': precision,
        'recall': recall
    }


def calculate_validation_metrics(predictions, targets, config):
    """è®¡ç®—éªŒè¯æŒ‡æ ‡ - ä¸¥æ ¼ç‰ˆæœ¬"""
    try:
        print("ğŸ“Š è®¡ç®—éªŒè¯æŒ‡æ ‡...")

        total_predictions = len(predictions)
        total_targets = len(targets)

        if total_predictions == 0 or total_targets == 0:
            return {
                'mAP50': 0.0,
                'mAP50-95': 0.0,
                'precision': 0.0,
                'recall': 0.0,
                'TP': 0,
                'FP': 0,
                'FN': 0
            }

        # è®¡ç®—ä¸¥æ ¼çš„TPã€FPã€FN
        strict_metrics = calculate_strict_metrics(predictions, targets, iou_threshold=0.5)

        # ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„é¢„æµ‹å’ŒçœŸå®æ•°é‡ï¼ˆç”¨äºç®€åŒ–çš„mAPè®¡ç®—ï¼‰
        pred_counts = {}
        target_counts = {}

        for pred in predictions:
            if 'labels' in pred and len(pred['labels']) > 0:
                labels = pred['labels'].cpu().numpy() if hasattr(pred['labels'], 'cpu') else pred['labels']
                for label in labels:
                    pred_counts[label] = pred_counts.get(label, 0) + 1

        for target in targets:
            if 'labels' in target and len(target['labels']) > 0:
                labels = target['labels'].cpu().numpy() if hasattr(target['labels'], 'cpu') else target['labels']
                for label in labels:
                    target_counts[label] = target_counts.get(label, 0) + 1

        # ç®€åŒ–çš„mAPè®¡ç®—ï¼ˆä¿æŒåŸæœ‰é€»è¾‘ï¼‰
        class_aps = []
        for class_id in range(1, config['model']['num_classes']):
            pred_count = pred_counts.get(class_id, 0)
            target_count = target_counts.get(class_id, 0)

            if target_count > 0:
                # åŸºäºä¸¥æ ¼recallçš„APä¼°ç®—
                class_recall = strict_metrics['recall']
                ap = min(pred_count / target_count, 1.0) * class_recall
            else:
                ap = 0.0
            class_aps.append(ap)

        # è®¡ç®—å¹³å‡å€¼
        mAP50 = sum(class_aps) / len(class_aps) if class_aps else 0.0
        mAP50_95 = mAP50 * 0.8  # ç®€åŒ–ä¼°ç®—

        print(f"âœ… éªŒè¯æŒ‡æ ‡è®¡ç®—å®Œæˆ - mAP50: {mAP50:.4f}, Precision: {strict_metrics['precision']:.4f}, Recall: {strict_metrics['recall']:.4f}")
        print(f"   TP: {strict_metrics['TP']}, FP: {strict_metrics['FP']}, FN: {strict_metrics['FN']}")

        return {
            'mAP50': mAP50,
            'mAP50-95': mAP50_95,
            'precision': strict_metrics['precision'],
            'recall': strict_metrics['recall'],
            'TP': strict_metrics['TP'],
            'FP': strict_metrics['FP'],
            'FN': strict_metrics['FN']
        }

    except Exception as e:
        print(f"è®¡ç®—éªŒè¯æŒ‡æ ‡æ—¶å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return {
            'mAP50': 0.0,
            'mAP50-95': 0.0,
            'precision': 0.0,
            'recall': 0.0,
            'TP': 0,
            'FP': 0,
            'FN': 0
        }


def validate_config(config):
    """éªŒè¯é…ç½®æ–‡ä»¶çš„æœ‰æ•ˆæ€§"""
    required_keys = [
        'model.backbone',
        'model.num_classes',
        'training.epochs',
        'training.batch_size',
        'training.learning_rate'
    ]

    for key in required_keys:
        keys = key.split('.')
        current = config
        try:
            for k in keys:
                current = current[k]
        except KeyError:
            raise ValueError(f"é…ç½®æ–‡ä»¶ç¼ºå°‘å¿…éœ€çš„é”®: {key}")

    # æ£€æŸ¥backboneæ”¯æŒ
    supported_backbones = [
        'resnet50', 'resnet50_v2', 'resnet101',
        'mobilenet_v3_large', 'mobilenet_v3_large_320'
    ]
    if config['model']['backbone'] not in supported_backbones:
        raise ValueError(f"ä¸æ”¯æŒçš„backbone: {config['model']['backbone']}")

    print("âœ… é…ç½®æ–‡ä»¶éªŒè¯é€šè¿‡")


def main():
    """ä¸»è®­ç»ƒå‡½æ•°"""
    # åŠ è½½é…ç½®
    with open("config.yaml", "r") as f:
        config = yaml.safe_load(f)

    # éªŒè¯é…ç½®
    validate_config(config)

    # è®¾ç½®éšæœºç§å­
    set_seed(config["seed"])

    # è·å–è®¾å¤‡
    device = get_device(config["device"])
    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # æ£€æŸ¥æ•°æ®é›†æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è½¬æ¢
    train_annotations = os.path.join(
        config["convert_output_path"], config["train_annotations"]
    )

    if not os.path.exists(train_annotations):
        print("æœªæ‰¾åˆ°COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶ï¼Œå¼€å§‹è½¬æ¢æ•°æ®é›†...")
        convert_visdrone_to_coco(
            visdrone_dir=config['dataset_path'], output_dir=config["convert_output_path"]
        )

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    print("åˆ›å»ºæ•°æ®åŠ è½½å™¨...")
    train_loader, val_loader = create_data_loaders(config)

    # åˆ›å»ºæ¨¡å‹
    print("åˆ›å»ºæ¨¡å‹...")
    model = create_model(config, device)
    print_model_summary(model)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = create_optimizer(model, config)

    # åˆ›å»ºå­¦ä¹ ç‡è°ƒåº¦å™¨
    scheduler = create_scheduler(optimizer, config, len(train_loader))

    # åˆ›å»ºEMAæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
    use_ema = config.get("training", {}).get("use_ema", False)
    if use_ema:
        ema = ModelEMA(model)
        print("å¯ç”¨EMAæ¨¡å‹")

    # åˆ›å»ºæ—©åœ
    early_stopping = EarlyStopping(
        patience=config["training"].get("patience"),
        min_delta=config["training"].get("min_delta"),
    )

    # æ£€æŸ¥æ˜¯å¦æœ‰ä¹‹å‰çš„æ£€æŸ¥ç‚¹
    latest_checkpoint = find_latest_checkpoint()
    start_epoch = 0
    best_loss = float("inf")
    resume_training = False

    if latest_checkpoint:
        print(f"ğŸ”„ æ‰¾åˆ°å·²æœ‰æ£€æŸ¥ç‚¹: {latest_checkpoint}")
        response = input("æ˜¯å¦ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒ? (Y/n): ").strip().lower()
        if response in ['', 'y', 'yes']:
            start_epoch, best_loss = load_checkpoint(
                latest_checkpoint, model, optimizer, scheduler, device
            )
            resume_training = True
            print(f"ğŸ”„ ä»æ–­ç‚¹ç»§ç»­è®­ç»ƒï¼Œä»epoch {start_epoch}å¼€å§‹")
        else:
            print("ğŸ†• ä»é¢„è®­ç»ƒæ¨¡å‹å¼€å§‹æ–°è®­ç»ƒ")
    else:
        print("ğŸ†• æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹ï¼Œå¼€å§‹æ–°è®­ç»ƒ")

    # åˆ›å»ºTensorBoardå†™å…¥å™¨
    if config["logging"]["tensorboard"]:
        if resume_training:
            # å¦‚æœç»§ç»­è®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰çš„æ—¥å¿—ç›®å½•
            log_dir = os.path.dirname(latest_checkpoint).replace("checkpoints", "runs")
            if not os.path.exists(log_dir):
                log_dir = f"runs/faster_rcnn_{time.strftime('%Y%m%d_%H%M%S')}"
        else:
            log_dir = f"runs/faster_rcnn_{time.strftime('%Y%m%d_%H%M%S')}"

        writer = SummaryWriter(log_dir)
        print(f"TensorBoardæ—¥å¿—ä¿å­˜åœ¨: {log_dir}")

    # åˆ›å»ºä¿å­˜ç›®å½•
    if resume_training:
        # å¦‚æœç»§ç»­è®­ç»ƒï¼Œä½¿ç”¨ç°æœ‰çš„ä¿å­˜ç›®å½•
        save_dir = Path(os.path.dirname(latest_checkpoint))
    else:
        save_dir = Path(f"checkpoints/faster_rcnn_{time.strftime('%Y%m%d_%H%M%S')}")
        save_dir.mkdir(parents=True, exist_ok=True)

    # æ£€æŸ¥æ˜¯å¦ä¸ºæµ‹è¯•æ¨¡å¼ï¼Œè°ƒæ•´è®­ç»ƒè½®æ•°
    test_mode = config.get('test_mode', {})
    if test_mode.get('enabled', False):
        total_epochs = test_mode.get('epochs', 5)
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å¼: å°†è®­ç»ƒ {total_epochs} è½®")
    else:
        total_epochs = config["training"]["epochs"]

    print("å¼€å§‹è®­ç»ƒ...")
    for epoch in range(start_epoch, total_epochs):
        # è®­ç»ƒ
        train_metrics = train_one_epoch(
            model, train_loader, optimizer, device, epoch, config
        )

        # éªŒè¯
        val_loss, val_metrics = validate(model, val_loader, device, config)

        # æ›´æ–°å­¦ä¹ ç‡
        if isinstance(scheduler, ReduceLROnPlateau):
            scheduler.step(val_loss)
        else:
            scheduler.step()

        # æ›´æ–°EMA
        if use_ema:
            ema.update()

        # TensorBoard è®°å½• - è®°å½•æŸå¤±ã€å­¦ä¹ ç‡å’Œè¯„ä¼°æŒ‡æ ‡
        if config["logging"]["tensorboard"]:
            writer.add_scalar("Loss/Train", train_metrics["loss"], epoch)
            writer.add_scalar("Loss/Val", val_loss, epoch)
            writer.add_scalar("Loss/RPN", train_metrics["rpn_loss"], epoch)
            writer.add_scalar("Loss/ROI", train_metrics["roi_loss"], epoch)
            writer.add_scalar("LR", optimizer.param_groups[0]["lr"], epoch)
            writer.add_scalar("Metrics/mAP50", val_metrics['mAP50'], epoch)
            writer.add_scalar("Metrics/mAP50-95", val_metrics['mAP50-95'], epoch)
            writer.add_scalar("Metrics/Precision", val_metrics['precision'], epoch)
            writer.add_scalar("Metrics/Recall", val_metrics['recall'], epoch)
            writer.add_scalar("Metrics/TP", val_metrics['TP'], epoch)
            writer.add_scalar("Metrics/FP", val_metrics['FP'], epoch)
            writer.add_scalar("Metrics/FN", val_metrics['FN'], epoch)

        # æ‰“å°è®­ç»ƒä¿¡æ¯ - æ˜¾ç¤ºæŸå¤±å’Œè¯„ä¼°æŒ‡æ ‡
        print(f"Epoch {epoch+1}/{config['training']['epochs']}")
        print(f"Train Loss: {train_metrics['loss']:.4f}")
        print(f"Val Loss: {val_loss:.4f}")
        print(f"mAP50: {val_metrics['mAP50']:.4f}")
        print(f"mAP50-95: {val_metrics['mAP50-95']:.4f}")
        print(f"Precision: {val_metrics['precision']:.4f} (TP={val_metrics['TP']}, FP={val_metrics['FP']})")
        print(f"Recall: {val_metrics['recall']:.4f} (TP={val_metrics['TP']}, FN={val_metrics['FN']})")
        print(f"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}")
        print("-" * 50)

        # ä¿å­˜æœ€ä½³æ¨¡å‹ï¼ˆåŸºäºéªŒè¯æŸå¤±ï¼‰
        if val_loss < best_loss:
            best_loss = val_loss
            best_model_path = save_dir / "best_model.pth"
            save_model(model, optimizer, scheduler, epoch, val_loss, str(best_model_path), config)
            print(f"ğŸ’¾ ä¿å­˜æœ€ä½³æ¨¡å‹: {best_model_path}")

        # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
        if (epoch + 1) % config["logging"]["save_interval"] == 0:
            checkpoint_path = save_dir / f"checkpoint_epoch_{epoch+1}.pth"
            save_model(model, optimizer, scheduler, epoch, val_loss, str(checkpoint_path), config)
            print(f"ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")

        # æ—©åœæ£€æŸ¥
        if early_stopping(val_loss):
            print("ğŸ›‘ æ—©åœè§¦å‘ï¼Œåœæ­¢è®­ç»ƒ")
            break

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_model_path = save_dir / "final_model.pth"
    save_model(model, optimizer, scheduler, epoch, val_loss, str(final_model_path), config)
    print(f"ğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹: {final_model_path}")

    # å…³é—­TensorBoard
    if config["logging"]["tensorboard"]:
        writer.close()

    print("âœ… è®­ç»ƒå®Œæˆï¼")
    print(f"ğŸ“Š è®­ç»ƒæ—¥å¿—ä¿å­˜åœ¨: {log_dir}")
    print(f"ğŸ’¾ æ¨¡å‹ä¿å­˜åœ¨: {save_dir}")
    print("ğŸ” è¿è¡Œ 'python generate_map_curves.py' æ¥ç”Ÿæˆ mAP50 å˜åŒ–å›¾")


if __name__ == "__main__":
    main()
