
import os
import json
import torch
import numpy as np
from PIL import Image
import torchvision.transforms as transforms
from torch.utils.data import Dataset, DataLoader, Subset
import albumentations as A
from albumentations.pytorch import ToTensorV2
import cv2
import random
from typing import Dict, List, Tuple, Optional


class VisDroneDataset(Dataset):
    """VisDroneæ•°æ®é›†ç±»ï¼Œæ”¯æŒCOCOæ ¼å¼æ ‡æ³¨"""

    def __init__(self,
                 root_dir: str,
                 annotation_file: str,
                 transform=None,
                 augmentation_config: Optional[Dict] = None):
        """
        Args:
            root_dir: æ•°æ®é›†æ ¹ç›®å½•
            annotation_file: COCOæ ¼å¼æ ‡æ³¨æ–‡ä»¶è·¯å¾„
            transform: å›¾åƒå˜æ¢
            augmentation_config: æ•°æ®å¢å¼ºé…ç½®
        """
        self.root_dir = root_dir
        self.transform = transform
        self.augmentation_config = augmentation_config

        # åŠ è½½COCOæ ¼å¼æ ‡æ³¨
        with open(annotation_file, 'r') as f:
            self.coco_data = json.load(f)

        # åˆ›å»ºå›¾åƒIDåˆ°æ ‡æ³¨çš„æ˜ å°„
        self.img_to_anns = {}
        for ann in self.coco_data['annotations']:
            img_id = ann['image_id']
            if img_id not in self.img_to_anns:
                self.img_to_anns[img_id] = []
            self.img_to_anns[img_id].append(ann)

        # åˆ›å»ºç±»åˆ«IDæ˜ å°„
        self.cat_id_to_label = {}
        for cat in self.coco_data['categories']:
            self.cat_id_to_label[cat['id']] = cat['name']

        # å›¾åƒåˆ—è¡¨
        self.images = self.coco_data['images']

        # è®¾ç½®æ•°æ®å¢å¼º
        self.setup_augmentation()

    def setup_augmentation(self):
        """è®¾ç½®æ•°æ®å¢å¼º"""
        if self.augmentation_config and self.augmentation_config.get('enabled', False):
            # è·å–é…ç½®å‚æ•°
            scale_range = self.augmentation_config.get('scale', [0.8, 1.2])
            translate_range = self.augmentation_config.get('translate', [0.1, 0.1])

            # è®¡ç®—scale_limit (RandomScaleéœ€è¦çš„æ˜¯åå·®å€¼ï¼Œä¸æ˜¯èŒƒå›´)
            scale_limit = (scale_range[1] - 1.0) if isinstance(scale_range, list) else 0.2

            self.aug_transform = A.Compose([
                A.HorizontalFlip(p=self.augmentation_config.get('horizontal_flip', 0.5)),
                A.VerticalFlip(p=self.augmentation_config.get('vertical_flip', 0.0)),
                A.Rotate(
                    limit=self.augmentation_config.get('rotation', 10),
                    p=0.5,
                    border_mode=cv2.BORDER_CONSTANT
                    # valueå‚æ•°åœ¨æ–°ç‰ˆæœ¬ä¸­å·²ç§»é™¤ï¼Œä½¿ç”¨é»˜è®¤å¡«å……
                ),
                A.ColorJitter(
                    brightness=self.augmentation_config.get('brightness', 0.2),
                    contrast=self.augmentation_config.get('contrast', 0.2),
                    saturation=self.augmentation_config.get('saturation', 0.2),
                    hue=self.augmentation_config.get('hue', 0.1),
                    p=0.5
                ),
                # ä¿®å¤RandomScale - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°æ ¼å¼
                A.RandomScale(
                    scale_limit=scale_limit,  # å•ä¸ªå€¼ï¼Œè¡¨ç¤ºç¼©æ”¾çš„æœ€å¤§åå·®
                    p=0.5
                ),
                # ä¿®å¤ShiftScaleRotate - ä½¿ç”¨æ­£ç¡®çš„å‚æ•°
                A.Affine(
                    translate_percent=translate_range[0] if isinstance(translate_range, list) else 0.1,
                    scale=1.0,  # ä¸è¿›è¡Œç¼©æ”¾ï¼Œå·²ç»ç”¨RandomScaleäº†
                    rotate=0,   # ä¸è¿›è¡Œæ—‹è½¬ï¼Œå·²ç»ç”¨Rotateäº†
                    p=0.5,
                    border_mode=cv2.BORDER_CONSTANT  # ä¿®å¤ï¼šä½¿ç”¨border_modeè€Œä¸æ˜¯mode
                    # fillå‚æ•°æ›¿ä»£äº†æ—§çš„valueå‚æ•°
                ),
                # æ·»åŠ æ›´å¤šå¢å¼ºé€‰é¡¹
                A.RandomBrightnessContrast(
                    brightness_limit=0.1,
                    contrast_limit=0.1,
                    p=0.3
                ),
                A.GaussNoise(
                    std_range=(0.01, 0.05),  # ä¿®å¤ï¼šä½¿ç”¨std_rangeå‚æ•°è€Œä¸æ˜¯noise_scale_factor
                    noise_scale_factor=1.0,  # å™ªå£°ç¼©æ”¾å› å­ï¼Œå•ä¸ªfloatå€¼
                    p=0.2
                ),
                A.Blur(
                    blur_limit=3,
                    p=0.1
                )
            ], bbox_params=A.BboxParams(
                format='pascal_voc',
                label_fields=['labels'],
                min_visibility=0.3,  # ç¡®ä¿å¢å¼ºåçš„æ¡†è‡³å°‘æœ‰30%å¯è§
                min_area=100  # æœ€å°æ¡†é¢ç§¯
            ))
        else:
            self.aug_transform = None

    def __len__(self):
        return len(self.images)

    def __getitem__(self, idx):
        # è·å–å›¾åƒä¿¡æ¯
        img_info = self.images[idx]
        img_path = os.path.join(self.root_dir, img_info['file_name'])

        # åŠ è½½å›¾åƒ
        image = cv2.imread(img_path)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        # è·å–è¯¥å›¾åƒçš„æ ‡æ³¨
        img_id = img_info['id']
        annotations = self.img_to_anns.get(img_id, [])

        # å‡†å¤‡è¾¹ç•Œæ¡†å’Œæ ‡ç­¾
        boxes = []
        labels = []

        for ann in annotations:
            bbox = ann['bbox']  # [x, y, width, height]
            # è½¬æ¢ä¸º [x1, y1, x2, y2] æ ¼å¼
            x1, y1, w, h = bbox
            x2, y2 = x1 + w, y1 + h
            boxes.append([x1, y1, x2, y2])
            labels.append(ann['category_id'])

        # è½¬æ¢ä¸ºnumpyæ•°ç»„
        boxes = np.array(boxes, dtype=np.float32)
        labels = np.array(labels, dtype=np.int64)

        # åº”ç”¨æ•°æ®å¢å¼º
        if self.aug_transform and len(boxes) > 0:
            augmented = self.aug_transform(image=image, bboxes=boxes, labels=labels)
            image = augmented['image']
            boxes = np.array(augmented['bboxes'], dtype=np.float32)
            labels = np.array(augmented['labels'], dtype=np.int64)

        # åº”ç”¨å˜æ¢
        if self.transform:
            image = self.transform(image)

        # è½¬æ¢ä¸ºtensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        # åˆ›å»ºç›®æ ‡å­—å…¸
        target = {
            'boxes': boxes,
            'labels': labels,
            'image_id': torch.tensor([img_id]),
            'area': torch.as_tensor([(box[2] - box[0]) * (box[3] - box[1]) for box in boxes], dtype=torch.float32),
            'iscrowd': torch.zeros((len(boxes),), dtype=torch.int64)
        }

        return image, target


def get_transform(train: bool, config: Dict):
    """è·å–å›¾åƒå˜æ¢"""
    if train:
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=config['model']['transform']['image_mean'],
                std=config['model']['transform']['image_std']
            )
        ])
    else:
        transform = transforms.Compose([
            transforms.ToPILImage(),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=config['model']['transform']['image_mean'],
                std=config['model']['transform']['image_std']
            )
        ])

    return transform


def collate_fn(batch):
    """è‡ªå®šä¹‰collateå‡½æ•°ï¼Œå¤„ç†ä¸åŒæ•°é‡çš„ç›®æ ‡"""
    return tuple(zip(*batch))


def create_data_loaders(config: Dict):
    """åˆ›å»ºæ•°æ®åŠ è½½å™¨"""
    # è·å–å˜æ¢
    train_transform = get_transform(train=True, config=config)
    val_transform = get_transform(train=False, config=config)

    # åˆ›å»ºæ•°æ®é›†
    train_dataset = VisDroneDataset(
        root_dir=os.path.join(config['convert_output_path'], config['train_images']),
        annotation_file=os.path.join(config['convert_output_path'], config['train_annotations']),
        transform=train_transform,
        augmentation_config=config.get('augmentation', {})
    )

    val_dataset = VisDroneDataset(
        root_dir=os.path.join(config['convert_output_path'], config['val_images']),
        annotation_file=os.path.join(config['convert_output_path'], config['val_annotations']),
        transform=val_transform
    )

    # æ£€æŸ¥æ˜¯å¦å¯ç”¨æµ‹è¯•æ¨¡å¼
    test_mode = config.get('test_mode', {})
    if test_mode.get('enabled', False):
        print("ğŸ§ª æµ‹è¯•æ¨¡å¼å·²å¯ç”¨")

        # è·å–æµ‹è¯•æ¨¡å¼å‚æ•°
        max_train_samples = test_mode.get('max_train_samples', 100)
        max_val_samples = test_mode.get('max_val_samples', 50)
        test_batch_size = test_mode.get('batch_size', 2)
        test_num_workers = test_mode.get('num_workers', 2)

        # åˆ›å»ºéšæœºç´¢å¼•ï¼ˆç¡®ä¿å¯é‡ç°ï¼‰
        random.seed(config.get('seed', 42))

        # é™åˆ¶è®­ç»ƒé›†å¤§å°
        if len(train_dataset) > max_train_samples:
            train_indices = random.sample(range(len(train_dataset)), max_train_samples)
            train_dataset = Subset(train_dataset, train_indices)
            print(f"ğŸ“Š è®­ç»ƒé›†å·²é™åˆ¶ä¸º {max_train_samples} ä¸ªæ ·æœ¬")

        # é™åˆ¶éªŒè¯é›†å¤§å°
        if len(val_dataset) > max_val_samples:
            val_indices = random.sample(range(len(val_dataset)), max_val_samples)
            val_dataset = Subset(val_dataset, val_indices)
            print(f"ğŸ“Š éªŒè¯é›†å·²é™åˆ¶ä¸º {max_val_samples} ä¸ªæ ·æœ¬")

        # ä½¿ç”¨æµ‹è¯•æ¨¡å¼çš„æ‰¹æ¬¡å¤§å°å’Œå·¥ä½œè¿›ç¨‹æ•°
        train_batch_size = test_batch_size
        val_batch_size = test_batch_size
        num_workers = test_num_workers

        print(f"ğŸ“Š æµ‹è¯•æ¨¡å¼é…ç½®:")
        print(f"  - è®­ç»ƒæ ·æœ¬æ•°: {len(train_dataset)}")
        print(f"  - éªŒè¯æ ·æœ¬æ•°: {len(val_dataset)}")
        print(f"  - æ‰¹æ¬¡å¤§å°: {test_batch_size}")
        print(f"  - å·¥ä½œè¿›ç¨‹æ•°: {test_num_workers}")

    else:
        # æ­£å¸¸æ¨¡å¼
        train_batch_size = config['training']['batch_size']
        val_batch_size = config['validation']['batch_size']
        num_workers = config['training']['num_workers']

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    train_loader = DataLoader(
        train_dataset,
        batch_size=train_batch_size,
        shuffle=True,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )

    val_loader = DataLoader(
        val_dataset,
        batch_size=val_batch_size,
        shuffle=False,
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=True
    )

    return train_loader, val_loader
